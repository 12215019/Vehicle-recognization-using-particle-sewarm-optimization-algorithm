{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1252089,"sourceType":"datasetVersion","datasetId":719458}],"dockerImageVersionId":29981,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn import metrics\n\ntrain_dir = '../input/vehicle-data-set/cardataset/train'  # Defining directory containing training data\ntest_dir = '../input/vehicle-data-set/cardataset/test'  # Definining directory containing test data\n\nnum_classes = len(os.listdir(train_dir))  # Counting the number of classes in the dataset\n\nheight = 150  # Setting the height of the images\nwidth = 150  # Setting the width of the images\nchannels = 3  # Setting the number of color channels\n\nbatch_size = 128  # Defining batch size for training\nepochs = 5  # Defining number of epochs for training\n\n# Createing data generators for training and validation data\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=40,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir, \n    target_size=(height, width),\n    batch_size=batch_size,\n    seed=1337,\n    shuffle=True,\n    class_mode='categorical')\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\nvalidation_generator = test_datagen.flow_from_directory(\n    test_dir, \n    target_size=(height, width), \n    batch_size=batch_size,\n    seed=1337,\n    shuffle=True,\n    class_mode='categorical')\n\ntrain_num = train_generator.samples  # Getting the number of training samples\nvalidation_num = validation_generator.samples  # Getting the number of validation samples\n\nclass Particle:\n    \"\"\"Class representing a particle in the PSO algorithm.\"\"\"\n    def __init__(self, bounds):\n        self.position = np.random.uniform(bounds[0], bounds[1], size=len(bounds))\n        self.velocity = np.random.uniform(-1, 1, size=len(bounds))\n        self.best_position = self.position\n        self.best_score = float('inf')\n\ndef pso(cost_function, bounds, num_particles, max_iter):\n    \"\"\"Implement Particle Swarm Optimization algorithm.\"\"\"\n    swarm = [Particle(bounds) for _ in range(num_particles)]\n    global_best_position = np.zeros_like(bounds[0])\n    global_best_score = float('inf')\n\n    for _ in range(max_iter):\n        for particle in swarm:\n            score = cost_function(particle.position)\n            if score < particle.best_score:\n                particle.best_score = score\n                particle.best_position = particle.position\n            if score < global_best_score:\n                global_best_score = score\n                global_best_position = particle.position\n\n            w = 0.5  # Seting inertia weight\n            c1 = 1.5  # Seting cognitive weight\n            c2 = 1.5  # Seting social weight\n            particle.velocity = (w * particle.velocity) + (c1 * np.random.uniform() * (particle.best_position - particle.position)) + (c2 * np.random.uniform() * (global_best_position - particle.position))\n            particle.position += particle.velocity\n\n    return global_best_position\n\ndef train_neural_network(params):\n    \"\"\"Train a neural network using the provided hyperparameters.\"\"\"\n    model = Sequential()\n    model.add(Conv2D(int(params[0]), (3, 3), input_shape=(height, width, channels)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Conv2D(int(params[0]), (3, 3)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(int(params[1]), activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n\n    history = model.fit_generator(\n        train_generator,\n        steps_per_epoch=train_num // batch_size,\n        epochs=epochs,\n        validation_data=validation_generator,\n        validation_steps=validation_num // batch_size,\n        verbose=1)\n\n    return history.history['val_acc'][-1]\n\nbounds = [(16, 64), (64, 512)]  # Defining hyperparameter boundaries\nnum_particles = 10  # Seting the number of particles in the PSO algorithm\nmax_iter = 10  # Seting the maximum number of iterations for PSO\n\nbest_hyperparameters = pso(train_neural_network, bounds, num_particles, max_iter)\n\nmodel = Sequential()\nmodel.add(Conv2D(int(best_hyperparameters[0]), (3, 3), input_shape=(height, width, channels)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(int(best_hyperparameters[0]), (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(int(best_hyperparameters[1]), activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=train_num // batch_size,\n    epochs=epochs,\n    validation_data=validation_generator,\n    validation_steps=validation_num // batch_size,\n    verbose=1)\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\nplt.title('Training and validation accuracy')\nplt.plot(epochs, acc, 'red', label='Training acc')\nplt.plot(epochs, val_acc, 'blue', label='Validation acc')\nplt.legend()\nplt.show()\n\nplt.title('Training and validation loss')\nplt.plot(epochs, loss, 'red', label='Training loss')\nplt.plot(epochs, val_loss, 'blue', label='Validation loss')\nplt.legend()\nplt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-06T12:23:27.606296Z","iopub.execute_input":"2024-04-06T12:23:27.606836Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Found 22852 images belonging to 17 classes.\nFound 5193 images belonging to 17 classes.\nEpoch 1/5\n 92/178 [==============>...............] - ETA: 5:26 - loss: 2.0645 - acc: 0.3820","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]}]}